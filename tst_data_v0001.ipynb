{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRY1nQmQ+ylTQLOlFRkWus",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtrochepy/astronomer/blob/main/tst_data_v0001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRGLKSmVpAqS",
        "outputId": "d6aac7ff-5521-4db1-b103-d0c225e0fab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install tqdm\n",
        "# !pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from joblib import dump, load\n",
        "from scipy.stats import skew, kurtosis\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.combine import SMOTEENN\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "QX6tS_-vp297"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignorar advertencias de pandas para una ejecución más limpia\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuración de pandas para mostrar todas las columnas al imprimir\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "Rf10krFhp6cK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lee el archivo CSV.  Error handling mejorado.\n",
        "try:\n",
        "    df = pd.read_csv(\"data_labels.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: El archivo 'data_labels.csv' no se encuentra.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(\"Error: El archivo 'data_labels.csv' está vacío.\")\n",
        "except pd.errors.ParserError:\n",
        "    print(\"Error: Error al analizar el archivo 'data_labels.csv'.\")"
      ],
      "metadata": {
        "id": "qfk34MyDqATy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ver cuantas filas y columnas tiene (90009 filas, 191 columnas)\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYl02HUZqHYK",
        "outputId": "f1c4ca21-633a-4d88-9b5e-dae34b78f2ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90009, 191)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Filas cargadas: {len(df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMcHD8aKqW55",
        "outputId": "af5cbdf1-9887-426a-d4cc-891815b4b074"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filas cargadas: 90009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_missing_values(df, threshold=0.5):\n",
        "    # Elimina columnas con más del umbral de valores nulos\n",
        "    df = df.dropna(thresh=threshold * len(df), axis=1)\n",
        "\n",
        "    # Identificar columnas numéricas\n",
        "    numerical_columns = df.select_dtypes(include=['number']).columns\n",
        "    # Calcular porcentaje de nulos\n",
        "    null_percentage = df[numerical_columns].isnull().mean() * 100\n",
        "\n",
        "    # Clasificar columnas por porcentaje de nulos\n",
        "    low_null = null_percentage[null_percentage < 10].index\n",
        "    mid_null = null_percentage[(null_percentage >= 10) & (null_percentage < 30)].index\n",
        "    high_null = null_percentage[null_percentage >= 30].index\n",
        "\n",
        "    # Relleno de nulos\n",
        "    df[low_null] = df[low_null].fillna(df[low_null].mean())\n",
        "    df[mid_null] = df[mid_null].fillna(df[mid_null].median())\n",
        "    if high_null.any():\n",
        "        imputer = IterativeImputer()\n",
        "        df[high_null] = imputer.fit_transform(df[high_null])\n",
        "\n",
        "    return df\n",
        "\n",
        "df = handle_missing_values(df)\n"
      ],
      "metadata": {
        "id": "MnUF9oCqtxks"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "def binarize_levels_with_missing(value):\n",
        "    # Ejemplo de lógica para binarización\n",
        "    if value == 'Missing':\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def process_categorical(df, categorical_columns, binary_columns):\n",
        "    # Rellenar nulos con 'Missing' y agregar 'Missing' como categoría si no existe\n",
        "    for col in categorical_columns:\n",
        "        if df[col].dtype.name == 'category':\n",
        "            if 'Missing' not in df[col].cat.categories:\n",
        "                df[col] = df[col].cat.add_categories(['Missing'])\n",
        "            df[col] = df[col].fillna('Missing')\n",
        "        else:\n",
        "            df[col] = df[col].fillna('Missing').astype('category')\n",
        "\n",
        "    # Aplicar binning personalizado si aplica\n",
        "    for col in ['Infraction_CLH', 'Base_67254', 'Infraction_TEN']:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].apply(binarize_levels_with_missing)\n",
        "\n",
        "    # One-Hot Encoding\n",
        "    df = pd.get_dummies(df, columns=categorical_columns, drop_first=False)\n",
        "\n",
        "    # Label Encoding en columnas binarias\n",
        "    le = LabelEncoder()\n",
        "    for col in binary_columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Ejemplo de uso (asegúrate de tener un DataFrame válido en df)\n",
        "categorical_columns = ['Infraction_YFSG', 'Infraction_DQLY', 'Infraction_CLH',\n",
        "                       'Base_67254', 'Infraction_TEN', 'Base_8730', 'Base_23737',\n",
        "                       'Infraction_NMCB', 'Infraction_ZRH', 'Infraction_WIS', 'Infraction_WMAQ']\n",
        "binary_columns = ['Base_23737', 'Infraction_NMCB', 'Infraction_ZRH', 'Infraction_WIS']\n",
        "\n",
        "# Ejecuta la función en tu DataFrame\n",
        "df = process_categorical(df, categorical_columns, binary_columns)"
      ],
      "metadata": {
        "id": "-yx95O66t1cc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_outliers(df, numeric_columns, iqr_multiplier=1.5):\n",
        "    Q1 = df[numeric_columns].quantile(0.25)\n",
        "    Q3 = df[numeric_columns].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Filtrar y manejar outliers\n",
        "    for col in numeric_columns:\n",
        "        lower_limit = Q1[col] - iqr_multiplier * IQR[col]\n",
        "        upper_limit = Q3[col] + iqr_multiplier * IQR[col]\n",
        "        df[col] = np.clip(df[col], lower_limit, upper_limit)\n",
        "\n",
        "    return df\n",
        "\n",
        "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "df = handle_outliers(df, numeric_columns)"
      ],
      "metadata": {
        "id": "q-ngD3eQvGX0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convirtiendo la columna 'fecha' a datetime\n",
        "df['Expenditure_AHF'] = pd.to_datetime(df['Expenditure_AHF'])"
      ],
      "metadata": {
        "id": "jCWlTYCSzc3X"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conversión de columnas no numéricas, ejemplo para fechas\n",
        "# df['Expenditure_AHF'] = pd.to_datetime(df['Expenditure_AHF'])\n",
        "# df['Expenditure_AHF'] = (df['Expenditure_AHF'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1d')"
      ],
      "metadata": {
        "id": "glm7A8DXzfTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.combine import SMOTEENN\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def balance_data(X, y, method='ADASYN'):\n",
        "    if method == 'ADASYN':\n",
        "        sampler = ADASYN(random_state=42)\n",
        "    elif method == 'SMOTEENN':\n",
        "        sampler = SMOTEENN(random_state=42)\n",
        "    else:\n",
        "        raise ValueError(\"Método de balanceo no soportado.\")\n",
        "\n",
        "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "# Verificar y convertir columnas no numéricas\n",
        "def preprocess_dataframe(df):\n",
        "    # Convertir fechas a números (días desde el 01-01-1970)\n",
        "    for col in df.select_dtypes(include=['datetime64']).columns:\n",
        "        df[col] = (df[col] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1d')\n",
        "\n",
        "    # Aplicar One-Hot Encoding para columnas categóricas\n",
        "    df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "    # Convertir todas las columnas a tipo float\n",
        "    df = df.astype(float)\n",
        "\n",
        "    # Verificar que todas las columnas sean numéricas\n",
        "    non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns\n",
        "    if len(non_numeric_cols) > 0:\n",
        "        raise ValueError(f\"Existen columnas no numéricas: {list(non_numeric_cols)}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Simulación: conversión del DataFrame\n",
        "df['Expenditure_AHF'] = pd.to_datetime(df['Expenditure_AHF'])\n",
        "df = preprocess_dataframe(df)\n",
        "\n",
        "# Separar características y etiquetas\n",
        "X = df.drop(columns=['label'])\n",
        "y = df['label']\n",
        "\n",
        "# División en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Balancear los datos de entrenamiento\n",
        "X_train_balanced, y_train_balanced = balance_data(X_train, y_train, method='ADASYN')"
      ],
      "metadata": {
        "id": "4mAJwIdBvKHr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}